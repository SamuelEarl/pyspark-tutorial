{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2455a82c-045e-4751-8a1e-564c6c6578e8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**This tutorial will cover:**\n",
    "\n",
    "* Setting up a PySpark session and some basic PySpark functions\n",
    "* Comparing PySpark to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b84538-5134-47c6-a3b5-e785fd3377d4",
   "metadata": {},
   "source": [
    "## Setting up a PySpark session and some basic PySpark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977ecbde-6692-4270-93ab-4080e6b9e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Docker container already contains PySpark preinstalled, there is no need to install it here.\n",
    "# Prefix shell commands with an exclamation mark.\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a838f8e-b37a-4413-9f63-168fe7aab270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the package has been installed properly, then an import will NOT throw an error.\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f96b77-e944-4ddd-8287-f836aa93e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to read data with PySpark.\n",
    "# Before you can read data with PySpark you need to start a Spark session first.\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05448ba-667d-487c-89b2-572bc2e7b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a name for the applicaton and create a SparkSession. If a SparkSession already exists, then getOrCreate() will retrieve it.\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34329555-ed3b-4e16-a823-e1e402b6ac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c53ff27359fb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f01b8066740>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b2ca17-1bf9-4a66-9049-d694f4d377a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a dataset and return a DataFrameReader, which is an interface to read data from external data sources into a DataFrame.\n",
    "df_pyspark_no_header = spark.read.csv(\"test1.csv\")\n",
    "\n",
    "# Tip: you can type `spark.read.` and press the Tab key to see a list of possible read methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b616ff-6a00-410f-8472-eb43c273922c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataframe's schema. \n",
    "df_pyspark_no_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8086265-e984-4245-9d3e-f1bbe62063dd",
   "metadata": {},
   "source": [
    "That schema looks strange. Let's view the entire dataset as a dataframe with the `show()` method to see what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05628df1-7781-4cd1-9ea2-26cc2d1ec83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  _c0|_c1|\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "|Steve| 30|\n",
      "| Bill| 31|\n",
      "| John| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_no_header.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88863dc-e6b2-472a-bd74-e9649722177a",
   "metadata": {},
   "source": [
    "By default a PySpark dataframe uses `_c0`, `_c1`, etc as column headers. Let's use the first row as the header. We do that with the `option(\"header\", True)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24221bf5-ca0e-40ca-a60d-1a4b1557f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option(\"header\", True).csv(\"test1.csv\")\n",
    "\n",
    "# Tip: You can right-click on any word in the chain above and select \"Show Contextual Help\" (or press Ctrl+I) to view some code hints and tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b4653-81ae-44cb-8343-6bc0cb823bf1",
   "metadata": {},
   "source": [
    "Now if you view the dataframe's schema it might make more sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f6aa19b-e84d-4b7a-81c1-8de9204d0145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c607ee86-1656-4a02-933a-2b88714b0a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Steve| 30|\n",
      "| Bill| 31|\n",
      "| John| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ebc7c-a3c4-4a11-82ad-993f3004ec04",
   "metadata": {},
   "source": [
    "## Comparing PySpark to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388b3122-d6ff-49cb-8b79-fca5e65e7a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steve</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bill</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age\n",
       "0  Steve   30\n",
       "1   Bill   31\n",
       "2   John   32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to read data with Pandas.\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"test1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07222d03-057b-4d41-b0b2-9b2fe0188c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the type of a Pandas dataframe.\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b75739f-6336-4a74-b5fa-c83da37ef3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the type of a PySpark dataframe.\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb3e39-c59d-443b-9611-5081b83089fa",
   "metadata": {},
   "source": [
    "You can see that both the Pandas and PySpark dataframes are of type `DataFrame`. You will also see that Pandas and PySpark dataframes share some similar API features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8433ee7d-6192-4c40-adfb-858bcf9dfd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Steve', Age='30'),\n",
       " Row(Name='Bill', Age='31'),\n",
       " Row(Name='John', Age='32')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 3 rows of a PySpark dataframe in list format.\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6367344-5a8d-400c-85e9-1a43b99ec7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Steve| 30|\n",
      "| Bill| 31|\n",
      "+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show only the first two rows (instead of the entire dataset) as a dataframe.\n",
    "df_pyspark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad7ca90-cbdd-42f6-80d3-d175eee64c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View more details of the dataframe's schema.\n",
    "df_pyspark.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
