{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f6ebe6-72ea-4d0b-a80c-8bec5a37fe2b",
   "metadata": {},
   "source": [
    "# Tutorial 3\n",
    "\n",
    "**This tutorial will cover:**\n",
    "\n",
    "* Dropping columns\n",
    "* Dropping rows\n",
    "* Parameters for the dropping functions\n",
    "* Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59577ac7-df6e-42b8-933d-c6b44792ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254c8459-b00c-4d87-a35d-a0f2639d0a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| Name| Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Steve|  31|        10| 30000|\n",
      "| Bill|  30|         8| 25000|\n",
      "| John|  29|         4| 20000|\n",
      "| Paul|  24|         3| 20000|\n",
      "|Chris|  21|         1| 15000|\n",
      "|  Tom|  23|         2| 18000|\n",
      "|Frank|null|      null| 40000|\n",
      "| null|  34|        10| 38000|\n",
      "| null|  36|      null|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset.\n",
    "df1 = spark.read.csv(\"test-data-3.csv\", header=True, inferSchema=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7443b4-5ee8-45e1-85b4-184a8a31f38b",
   "metadata": {},
   "source": [
    "## Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7623509-86af-42fb-92df-dd280eb76540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"test-data-3.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a5133d-dd4a-4394-9114-5d9d3aece0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping columns does NOT mutate the original dataframe. Instead it returns a new dataframe with the specified columns removed.\n",
    "df2.drop(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4690fc6-89d5-4571-9521-9afe2695d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| Name| Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Steve|  31|        10| 30000|\n",
      "| Bill|  30|         8| 25000|\n",
      "| John|  29|         4| 20000|\n",
      "| Paul|  24|         3| 20000|\n",
      "|Chris|  21|         1| 15000|\n",
      "|  Tom|  23|         2| 18000|\n",
      "|Frank|null|      null| 40000|\n",
      "| null|  34|        10| 38000|\n",
      "| null|  36|      null|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So if you view the dataframe, you will still see the original dataframe.\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d626d23d-8d99-488a-ae88-f1b6e34d89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|null|      null| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In order to see the new dataframe you can save it to a variable and inspect it.\n",
    "df2_dropped_col = df2.drop(\"Name\")\n",
    "df2_dropped_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a568a92-eb14-4357-b2a4-60c15e0efbaf",
   "metadata": {},
   "source": [
    "## Dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09681ba9-3a9b-407d-a282-ba736ecc27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.csv(\"test-data-3.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e161859-d00b-4561-a558-a46e346ea625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Steve| 31|        10| 30000|\n",
      "| Bill| 30|         8| 25000|\n",
      "| John| 29|         4| 20000|\n",
      "| Paul| 24|         3| 20000|\n",
      "|Chris| 21|         1| 15000|\n",
      "|  Tom| 23|         2| 18000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all rows that have at least 1 null value.\n",
    "df3.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae897eb-f16e-42f5-89f8-03f30550b821",
   "metadata": {},
   "source": [
    "## Parameters for the dropping methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549004ac-060e-473d-a751-ddf51208ffbd",
   "metadata": {},
   "source": [
    "In the previous code cell, if you right-click `drop()` and select \"Show Contextual Help\", then you will see that `drop()` accepts three parameters: `how`, `thresh` (i.e. threshhold), and `subset`. You can read the descriptions for how each of those parameters works. However, the `subset` parameter might be a little confusing. When you pass a list of column names to `subset`, then the setting that you passed to the `how` or `thresh` parameters will only be applied to the columns that you specified. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb4ec56-707b-49e0-a2ff-fd0c982a45d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| Name| Age|Experience|Salary|\n",
      "+-----+----+----------+------+\n",
      "|Steve|  31|        10| 30000|\n",
      "| Bill|  30|         8| 25000|\n",
      "| John|  29|         4| 20000|\n",
      "| Paul|  24|         3| 20000|\n",
      "|Chris|  21|         1| 15000|\n",
      "|  Tom|  23|         2| 18000|\n",
      "|Frank|null|      null| 40000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.na.drop(how=\"any\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a272b82-b191-4f7c-a9d6-8684f63f4f86",
   "metadata": {},
   "source": [
    "## Filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4882c8-dbb8-481a-92b0-0b3e40617e22",
   "metadata": {},
   "source": [
    "You can use the `fill()` method to fill in missing values. The `fill()` method can take two parameters: `value` and `subset`. The `value` parameter is the value that will replace the `null` values. The `subset` parameter takes a list of column names that will be filled with the replacement values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaa12e8-da82-4995-8189-9960891e2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.csv(\"test-data-3.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829553c7-a072-43b0-a0c3-bfbd57ef6827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Steve|           31|           10|        30000|\n",
      "|         Bill|           30|            8|        25000|\n",
      "|         John|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|        Chris|           21|            1|        15000|\n",
      "|          Tom|           23|            2|        18000|\n",
      "|        Frank|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill(\"Missing Value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faac7df-65c9-4047-8107-63dde4363881",
   "metadata": {},
   "source": [
    "If you look at the schema of the dataframe, then you will see that all of the column data types are `string`, which is why `Missing Value` replaced all the `null` values in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b3cbaa-3def-4b21-bfcb-876bc5a3aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263b5ec-395f-4ae1-8a9b-044a5c1a6041",
   "metadata": {},
   "source": [
    "However, if you pass `inferSchema=True` to the `csv()` method, then `Missing Value` will only fill in other values that have a string data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a58ca1-fe9e-4786-aae6-5ebcf957dd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = spark.read.csv(\"test-data-3.csv\", header=True, inferSchema=True)\n",
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6e65a6f-36d3-4791-ac59-41c06b4ca4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|        Steve|  31|        10| 30000|\n",
      "|         Bill|  30|         8| 25000|\n",
      "|         John|  29|         4| 20000|\n",
      "|         Paul|  24|         3| 20000|\n",
      "|        Chris|  21|         1| 15000|\n",
      "|          Tom|  23|         2| 18000|\n",
      "|        Frank|null|      null| 40000|\n",
      "|Missing Value|  34|        10| 38000|\n",
      "|Missing Value|  36|      null|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.na.fill(\"Missing Value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a1cf5-cf5c-4006-b9ba-b80b06ab7021",
   "metadata": {},
   "source": [
    "You can replace null values in specific columns by passing a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ddefa47-5b14-4021-a41e-74c9815e365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+------+\n",
      "| Name|          Age|   Experience|Salary|\n",
      "+-----+-------------+-------------+------+\n",
      "|Steve|           31|           10| 30000|\n",
      "| Bill|           30|            8| 25000|\n",
      "| John|           29|            4| 20000|\n",
      "| Paul|           24|            3| 20000|\n",
      "|Chris|           21|            1| 15000|\n",
      "|  Tom|           23|            2| 18000|\n",
      "|Frank|Missing Value|Missing Value| 40000|\n",
      "| null|           34|           10| 38000|\n",
      "| null|           36|Missing Value|  null|\n",
      "+-----+-------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill(\"Missing Value\", [\"Age\",\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5122768-76a8-4fce-a59a-f1680f43f360",
   "metadata": {},
   "source": [
    "## Filling missing values with mean, median, or mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770891c5-9518-4ae3-bbec-28fa3ec7c43d",
   "metadata": {},
   "source": [
    "In statistics, imputation is the process of replacing missing data with substituted values. ([Wikipedia](https://en.wikipedia.org/wiki/Imputation_(statistics)))\n",
    "\n",
    "To get more details about the following example, you can read the PySpark docs for [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3749c8db-c1e4-4773-aa17-01aa0b9cbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input columns and replace the missing values with the mean. The input columns should be of numeric type.\n",
    "# The resulting dataframe will contain the original columns plus the output columns, which will contain the imputed values.\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Setup the imputer configuration.\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Age\", \"Experience\", \"Salary\"],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in [\"Age\", \"Experience\", \"Salary\"]]\n",
    ").setStrategy(\"mean\")\n",
    "\n",
    "# You can also pass \"median\" or \"mode\" to the `setStrategy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8d95476-3c5d-4271-8118-f8e689af4f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "| Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "|Steve|  31|        10| 30000|         31|                10|         30000|\n",
      "| Bill|  30|         8| 25000|         30|                 8|         25000|\n",
      "| John|  29|         4| 20000|         29|                 4|         20000|\n",
      "| Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|Chris|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Tom|  23|         2| 18000|         23|                 2|         18000|\n",
      "|Frank|null|      null| 40000|         28|                 5|         40000|\n",
      "| null|  34|        10| 38000|         34|                10|         38000|\n",
      "| null|  36|      null|  null|         36|                 5|         25750|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the imputation columns to the dataframe.\n",
    "imputer.fit(df5).transform(df5).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
